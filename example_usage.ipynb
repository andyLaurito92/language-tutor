{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if missing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package_name, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"‚úÖ {package_name} already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package_name}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"‚úÖ {package_name} installed successfully\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to install {package_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "# Install key packages\n",
    "packages = [\n",
    "    (\"python-dotenv\", \"dotenv\"),\n",
    "    (\"requests\", \"requests\"),\n",
    "]\n",
    "\n",
    "print(\"üîß Checking and installing required packages...\")\n",
    "all_installed = True\n",
    "for package, import_name in packages:\n",
    "    if not install_if_missing(package, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\nüéâ All required packages are available!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some packages could not be installed. Check your environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a51d18",
   "metadata": {},
   "source": [
    "# AI Language Tutor - Example Usage\n",
    "\n",
    "This notebook demonstrates how to use the AI Language Tutor components programmatically.\n",
    "\n",
    "## üö® Important: Kernel Selection\n",
    "\n",
    "**Before running any cells, make sure you have selected the correct kernel:**\n",
    "\n",
    "1. Look at the top-right corner of this notebook in VS Code\n",
    "2. Click on the kernel selector (should show something like \"Python 3.x.x\")\n",
    "3. Select **\"Language Tutor (Python 3.10.18)\"** from the list\n",
    "4. If you don't see this option, run: `./activate_env.sh` in the terminal first\n",
    "\n",
    "## üìã Quick Start Checklist\n",
    "\n",
    "- [ ] ‚úÖ Correct kernel selected: \"Language Tutor (Python 3.10.18)\"\n",
    "- [ ] üîë OpenAI API key set in `.env` file\n",
    "- [ ] üì¶ All dependencies installed (test in cell 2 below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403c93ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment configured for Ollama with mistral:latest\n",
      "üì¶ Imports successful!\n",
      "üîß Model Provider: ollama\n",
      "ü¶ô Ollama Model: mistral:latest\n",
      "üåê Ollama URL: http://localhost:11434\n",
      "‚úÖ Ready to start using your AI Language Tutor!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# ‚ö° Quick Fix: Set environment variables directly (no .env file needed)\n",
    "os.environ['MODEL_PROVIDER'] = 'ollama'\n",
    "os.environ['OLLAMA_MODEL'] = 'mistral:latest'  # Use your available model\n",
    "os.environ['OLLAMA_BASE_URL'] = 'http://localhost:11434'\n",
    "\n",
    "print(\"üîß Environment configured for Ollama with mistral:latest\")\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "\n",
    "# Test if we can import (will handle missing dotenv gracefully)\n",
    "try:\n",
    "    from src.utils.config import Config\n",
    "    from src.tutor.ai_tutor import AITutor\n",
    "    from src.tutor.lessons import LessonManager\n",
    "    from src.utils.database import ProgressTracker\n",
    "\n",
    "    print(\"üì¶ Imports successful!\")\n",
    "    print(f\"üîß Model Provider: {Config.MODEL_PROVIDER}\")\n",
    "    if Config.MODEL_PROVIDER == 'ollama':\n",
    "        print(f\"ü¶ô Ollama Model: {Config.OLLAMA_MODEL}\")\n",
    "        print(f\"üåê Ollama URL: {Config.OLLAMA_BASE_URL}\")\n",
    "    elif Config.MODEL_PROVIDER == 'openai':\n",
    "        print(f\"ü§ñ OpenAI Model: {Config.OPENAI_MODEL}\")\n",
    "\n",
    "    print(\"‚úÖ Ready to start using your AI Language Tutor!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üí° You may need to install missing packages.\")\n",
    "    print(\"üí° Try running: conda run -n language-tutor pip install python-dotenv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    print(\"üí° Make sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9076335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Configure .env for Your Available Models\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "print(\"üîß Configuring .env file for your available Ollama models...\")\n",
    "\n",
    "# Check what models you actually have\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        available_models = [model['name'] for model in models]\n",
    "        print(f\"üì¶ Available models: {available_models}\")\n",
    "        \n",
    "        # Use the first available model as default\n",
    "        if available_models:\n",
    "            recommended_model = available_models[0]\n",
    "            print(f\"üéØ Recommended model: {recommended_model}\")\n",
    "            \n",
    "            # Update .env file\n",
    "            env_file = Path('.env')\n",
    "            env_content = f\"\"\"# AI Language Tutor Configuration\n",
    "\n",
    "# Model Provider (choose 'openai' or 'ollama')\n",
    "MODEL_PROVIDER=ollama\n",
    "\n",
    "# Ollama Configuration (for local models)\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "OLLAMA_MODEL={recommended_model}\n",
    "\n",
    "# OpenAI Configuration (optional, for cloud models)\n",
    "# OPENAI_API_KEY=your_openai_api_key_here\n",
    "# OPENAI_MODEL=gpt-4\n",
    "\n",
    "# Speech Configuration\n",
    "# STT_PROVIDER=google  # or 'openai' if you have API key\n",
    "# TTS_PROVIDER=none    # or 'openai' if you have API key\n",
    "\"\"\"\n",
    "            \n",
    "            with open(env_file, 'w') as f:\n",
    "                f.write(env_content)\n",
    "            \n",
    "            print(f\"‚úÖ Created .env file with {recommended_model}\")\n",
    "            print(\"üí° You can edit .env to change models or add OpenAI API key\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå No models found. Install one with: ollama pull mistral\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå Ollama responded with error\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ùå Cannot connect to Ollama\")\n",
    "    print(\"üí° Make sure Ollama is running: ollama serve\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\nüé¨ Your .env file is now configured for local Ollama models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffebb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conda environment and dependencies\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"üîß Environment Information:\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# Check if we're in conda environment\n",
    "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'Not detected')\n",
    "print(f\"Conda environment: {conda_env}\")\n",
    "\n",
    "# Check Python path\n",
    "print(f\"Python path includes: {len(sys.path)} locations\")\n",
    "if 'anaconda3' in sys.executable:\n",
    "    print(\"‚úÖ Using Anaconda/Conda Python\")\n",
    "elif 'language-tutor' in sys.executable:\n",
    "    print(\"‚úÖ Using language-tutor environment\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Not using expected conda environment\")\n",
    "    print(\"üí° Please select 'Language Tutor (Python 3.10.18)' kernel in VS Code\")\n",
    "\n",
    "# Test key imports\n",
    "print(\"\\nüì¶ Testing key dependencies:\")\n",
    "dependencies = [\n",
    "    ('openai', lambda: __import__('openai').__version__),\n",
    "    ('langchain', lambda: __import__('langchain').__version__),\n",
    "    ('streamlit', lambda: __import__('streamlit').__version__),\n",
    "    ('speech_recognition', lambda: __import__('speech_recognition').__version__),\n",
    "    ('pyaudio', lambda: 'Available'),\n",
    "    ('dotenv', lambda: 'Available'),\n",
    "    ('numpy', lambda: __import__('numpy').__version__),\n",
    "    ('pandas', lambda: __import__('pandas').__version__),\n",
    "]\n",
    "\n",
    "all_good = True\n",
    "for name, version_func in dependencies:\n",
    "    try:\n",
    "        version = version_func()\n",
    "        print(f\"‚úÖ {name}: {version}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {name}: {e}\")\n",
    "        all_good = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {name}: Error getting version - {e}\")\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüéØ Environment is ready for AI Language Tutor!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some dependencies are missing. Please check your environment setup.\")\n",
    "    print(\"üí° Make sure you're using the 'Language Tutor (Python 3.10.18)' kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d7260",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f87d07",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting\n",
    "\n",
    "If you encounter import errors, please check:\n",
    "\n",
    "1. **Kernel Selection**: Make sure you've selected the \"Language Tutor (Python 3.10.18)\" kernel in VS Code\n",
    "2. **Environment Activation**: Ensure the conda environment is activated\n",
    "3. **Missing Dependencies**: Run the environment test in cell 2 above\n",
    "\n",
    "### Common Issues:\n",
    "- **ModuleNotFoundError**: Wrong kernel selected or environment not activated\n",
    "- **dotenv not found**: Either install python-dotenv or set environment variables manually\n",
    "- **OpenAI API Key**: Create a `.env` file or set the API key directly in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Environment Setup Options\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"Current Python executable:\", sys.executable)\n",
    "print(\"Available modules test:\")\n",
    "\n",
    "# Test critical imports\n",
    "modules_to_test = [\n",
    "    ('os', 'Built-in'),\n",
    "    ('sys', 'Built-in'), \n",
    "    ('pathlib', 'Built-in'),\n",
    "]\n",
    "\n",
    "try:\n",
    "    import dotenv\n",
    "    modules_to_test.append(('dotenv', 'Available'))\n",
    "except ImportError:\n",
    "    modules_to_test.append(('dotenv', 'NOT AVAILABLE - Install with: pip install python-dotenv'))\n",
    "\n",
    "for module, status in modules_to_test:\n",
    "    print(f\"  {module}: {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìù SETUP OPTIONS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Option 1: Create .env file (Recommended)\")\n",
    "print(\"   Create a file named '.env' in the project root with:\")\n",
    "print(\"   OPENAI_API_KEY=your_api_key_here\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Option 2: Set environment variable manually\")\n",
    "print(\"   Uncomment and modify the line below:\")\n",
    "print(\"   # os.environ['OPENAI_API_KEY'] = 'your_api_key_here'\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Option 3: Set via terminal\")\n",
    "print(\"   export OPENAI_API_KEY='your_api_key_here'\")\n",
    "\n",
    "print(\"\\nüîç Current environment variables:\")\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_key:\n",
    "    print(f\"   OPENAI_API_KEY: {'*' * (len(openai_key) - 4) + openai_key[-4:] if len(openai_key) > 4 else '***'}\")\n",
    "else:\n",
    "    print(\"   OPENAI_API_KEY: Not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Quick .env File Setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = Path('.env')\n",
    "env_example = Path('.env.example')\n",
    "\n",
    "if env_file.exists():\n",
    "    print(\"‚úÖ .env file already exists\")\n",
    "    with open(env_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        if 'OPENAI_API_KEY' in content and 'your_openai_api_key_here' not in content:\n",
    "            print(\"‚úÖ OpenAI API key appears to be set\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Please add your actual OpenAI API key to .env file\")\n",
    "else:\n",
    "    print(\"‚ùå No .env file found\")\n",
    "    \n",
    "    if env_example.exists():\n",
    "        print(\"üìã Copying .env.example to .env...\")\n",
    "        # Copy .env.example to .env\n",
    "        with open(env_example, 'r') as f:\n",
    "            content = f.read()\n",
    "        with open(env_file, 'w') as f:\n",
    "            f.write(content)\n",
    "        print(\"‚úÖ Created .env file from template\")\n",
    "        print(\"üìù Please edit .env file and add your OpenAI API key\")\n",
    "    else:\n",
    "        print(\"üìù Creating basic .env file...\")\n",
    "        with open(env_file, 'w') as f:\n",
    "            f.write(\"# OpenAI API Configuration\\n\")\n",
    "            f.write(\"OPENAI_API_KEY=your_openai_api_key_here\\n\")\n",
    "        print(\"‚úÖ Created basic .env file\")\n",
    "        print(\"üìù Please edit .env file and add your actual OpenAI API key\")\n",
    "\n",
    "print(f\"\\nüí° To edit .env file:\")\n",
    "print(f\"   1. Open .env in VS Code\")\n",
    "print(f\"   2. Replace 'your_openai_api_key_here' with your actual API key\")\n",
    "print(f\"   3. Save the file\")\n",
    "print(f\"   4. Restart this notebook kernel (VS Code: Kernel -> Restart)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e65a3",
   "metadata": {},
   "source": [
    "## ü¶ô Ollama Setup (Local Models)\n",
    "\n",
    "If you want to use **local models instead of OpenAI**, follow these steps:\n",
    "\n",
    "### Quick Setup:\n",
    "1. **Install Ollama**: Download from [ollama.ai](https://ollama.ai) or run `brew install ollama`\n",
    "2. **Start Ollama**: Run `ollama serve` in a terminal\n",
    "3. **Download a model**: Run `ollama pull llama3.1`\n",
    "4. **Update .env**: Set `MODEL_PROVIDER=ollama` and `OLLAMA_MODEL=llama3.1`\n",
    "\n",
    "### Recommended Models:\n",
    "- **llama3.1** - Best overall (4.7GB)\n",
    "- **qwen2** - Excellent multilingual (4.4GB) \n",
    "- **mistral** - Fast responses (4.1GB)\n",
    "\n",
    "See `OLLAMA_SETUP.md` for detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a30a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü¶ô Test Ollama Connection\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def check_ollama_status():\n",
    "    \"\"\"Check if Ollama is running and what models are available.\"\"\"\n",
    "    print(\"üîç Checking Ollama status...\")\n",
    "    \n",
    "    # Check if Ollama service is running\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags')\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            print(\"‚úÖ Ollama is running\")\n",
    "            print(f\"üì¶ Available models: {len(models)}\")\n",
    "            \n",
    "            if models:\n",
    "                print(\"\\nüìã Your models:\")\n",
    "                for model in models:\n",
    "                    name = model.get('name', 'Unknown')\n",
    "                    size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "                    print(f\"  ‚Ä¢ {name} ({size:.1f}GB)\")\n",
    "            else:\n",
    "                print(\"üì• No models installed yet\")\n",
    "                print(\"üí° Install a model with: ollama pull llama3.1\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Ollama service responded with error\")\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama\")\n",
    "        print(\"üí° Make sure Ollama is running: ollama serve\")\n",
    "        print(\"üí° Install Ollama from: https://ollama.ai\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking Ollama: {e}\")\n",
    "\n",
    "# Only run this if using Ollama\n",
    "if os.getenv('MODEL_PROVIDER', 'ollama') == 'ollama':\n",
    "    check_ollama_status()\n",
    "else:\n",
    "    print(f\"ü§ñ Using {os.getenv('MODEL_PROVIDER', 'openai')} as model provider\")\n",
    "    print(\"üí° To use Ollama, set MODEL_PROVIDER=ollama in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f066ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "\n",
    "# Try to load from .env file if available\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Loaded environment variables from .env file\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv not available, using manual environment setup\")\n",
    "    print(\"üí° You can set OPENAI_API_KEY manually:\")\n",
    "    print(\"   os.environ['OPENAI_API_KEY'] = 'your_api_key_here'\")\n",
    "\n",
    "# Alternative: Set your API key manually here if needed\n",
    "# os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    Config.validate_config()\n",
    "    print(\"‚úÖ Configuration valid\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    print(\"Please set your OPENAI_API_KEY in the .env file or manually above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46124a",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212922d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AI tutor with the configured model provider\n",
    "tutor = AITutor.from_config(Config)\n",
    "\n",
    "# Initialize lesson manager\n",
    "lesson_manager = LessonManager()\n",
    "\n",
    "# Initialize progress tracker\n",
    "progress_tracker = ProgressTracker()\n",
    "\n",
    "print(\"‚úÖ All components initialized\")\n",
    "print(f\"üîß Using {Config.MODEL_PROVIDER} as model provider\")\n",
    "\n",
    "if Config.MODEL_PROVIDER == 'ollama':\n",
    "    print(f\"ü¶ô Ollama model: {Config.OLLAMA_MODEL}\")\n",
    "    print(\"üí° Make sure Ollama is running: 'ollama serve'\")\n",
    "elif Config.MODEL_PROVIDER == 'openai':\n",
    "    print(f\"ü§ñ OpenAI model: {Config.OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f6988",
   "metadata": {},
   "source": [
    "## Start a Learning Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "language = \"Spanish\"\n",
    "difficulty = \"Beginner\"\n",
    "lesson_type = \"Conversation Practice\"\n",
    "\n",
    "# Get lesson content\n",
    "lessons = lesson_manager.get_lessons(\"conversation\", \"beginner\")\n",
    "lesson_data = lessons[0] if lessons else {}\n",
    "\n",
    "print(f\"Selected lesson: {lesson_data.get('title', 'General conversation')}\")\n",
    "print(f\"Description: {lesson_data.get('description', 'Practice conversation skills')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78088d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning context\n",
    "tutor.set_learning_context(language, difficulty, lesson_type, lesson_data)\n",
    "\n",
    "# Start tracking session\n",
    "user_id = \"example_user\"\n",
    "session_id = progress_tracker.start_session(user_id, language, lesson_type, difficulty)\n",
    "\n",
    "print(f\"Started session {session_id} for {language} {lesson_type} at {difficulty} level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae644ad9",
   "metadata": {},
   "source": [
    "## Generate Lesson Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lesson introduction\n",
    "intro = tutor.generate_lesson_introduction()\n",
    "print(\"üéì Tutor says:\")\n",
    "print(intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb26f8",
   "metadata": {},
   "source": [
    "## Interactive Conversation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d222964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example student input\n",
    "student_input = \"Hola, me llamo Juan. ¬øC√≥mo est√°s?\"\n",
    "\n",
    "# Process the input\n",
    "response_data = tutor.process_student_input(student_input, \"text\")\n",
    "\n",
    "print(f\"üë§ Student: {student_input}\")\n",
    "print(f\"üéì Tutor: {response_data['response']}\")\n",
    "print(f\"\\nüìä Feedback:\")\n",
    "feedback = response_data['feedback']\n",
    "print(f\"Grammar Score: {feedback.get('grammar_score', 'N/A')}/10\")\n",
    "print(f\"Vocabulary Level: {feedback.get('vocabulary_level', 'N/A')}\")\n",
    "if feedback.get('strengths'):\n",
    "    print(f\"Strengths: {', '.join(feedback['strengths'])}\")\n",
    "if feedback.get('suggestions'):\n",
    "    print(f\"Suggestions: {', '.join(feedback['suggestions'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the interaction\n",
    "progress_tracker.log_interaction(\n",
    "    session_id,\n",
    "    student_input,\n",
    "    response_data['response'],\n",
    "    int(response_data.get('confidence_score', 0.8) * 10)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Interaction logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff044e3e",
   "metadata": {},
   "source": [
    "## Generate Practice Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75565e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a practice exercise\n",
    "exercise = tutor.generate_practice_exercise()\n",
    "\n",
    "print(\"üìù Practice Exercise:\")\n",
    "print(f\"Title: {exercise.get('title', 'Practice')}\")\n",
    "print(f\"Type: {exercise.get('type', 'conversation')}\")\n",
    "print(f\"Instructions: {exercise.get('instructions', '')}\")\n",
    "print(f\"Content: {exercise.get('content', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4e929",
   "metadata": {},
   "source": [
    "## End Session and View Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the session with a score\n",
    "session_score = 8  # Example score out of 10\n",
    "progress_tracker.end_session(session_id, session_score)\n",
    "\n",
    "# Get lesson summary\n",
    "summary = tutor.get_lesson_summary()\n",
    "print(\"üìã Lesson Summary:\")\n",
    "print(f\"Summary: {summary.get('summary', '')}\")\n",
    "print(f\"Achievements: {', '.join(summary.get('achievements', []))}\")\n",
    "print(f\"Areas to improve: {', '.join(summary.get('areas_to_improve', []))}\")\n",
    "print(f\"Next steps: {', '.join(summary.get('next_steps', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View progress data\n",
    "progress_data = progress_tracker.get_user_progress(user_id)\n",
    "\n",
    "print(\"üìä User Progress:\")\n",
    "print(f\"Total Sessions: {progress_data['total_sessions']}\")\n",
    "print(f\"Total Time: {progress_data['total_time']} seconds ({progress_data['total_time']//60} minutes)\")\n",
    "print(f\"Average Score: {progress_data['average_score']:.1f}/10\")\n",
    "\n",
    "if progress_data['sessions']:\n",
    "    print(\"\\nSession Details:\")\n",
    "    for session in progress_data['sessions']:\n",
    "        print(f\"  {session['language']} - {session['lesson_type']} ({session['difficulty']}): \"\n",
    "              f\"{session['session_count']} sessions, avg score {session['average_score']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae8c49",
   "metadata": {},
   "source": [
    "## Lesson Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb634b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View available lessons\n",
    "print(\"Available Conversation Lessons:\")\n",
    "for difficulty in ['beginner', 'intermediate', 'advanced']:\n",
    "    lessons = lesson_manager.get_lessons('conversation', difficulty)\n",
    "    print(f\"\\n{difficulty.title()} Level:\")\n",
    "    for lesson in lessons:\n",
    "        print(f\"  - {lesson.get('title', 'Untitled')}: {lesson.get('description', 'No description')}\")\n",
    "        if lesson.get('topics'):\n",
    "            print(f\"    Topics: {', '.join(lesson['topics'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lesson topics for a language\n",
    "topics = lesson_manager.get_lesson_topics(\"Spanish\")\n",
    "print(f\"Available topics for Spanish: {', '.join(topics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d475b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Complete Ollama Integration\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "# Test both providers\n",
    "print(\"=== Testing Complete Integration ===\")\n",
    "\n",
    "# 1. Test OpenAI (if available)\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_key:\n",
    "    print(\"\\n1. Testing OpenAI Provider:\")\n",
    "    os.environ['MODEL_PROVIDER'] = 'openai'\n",
    "    \n",
    "    from src.utils.config import Config\n",
    "    from src.tutor.ai_tutor import AITutor\n",
    "    \n",
    "    try:\n",
    "        config = Config()\n",
    "        tutor = AITutor.from_config(config)\n",
    "        response = tutor.generate_response(\"Hello, can you help me learn Spanish?\")\n",
    "        print(f\"‚úÖ OpenAI Response: {response[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OpenAI Error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OpenAI API key not found, skipping OpenAI test\")\n",
    "\n",
    "# 2. Test Ollama\n",
    "print(\"\\n2. Testing Ollama Provider:\")\n",
    "os.environ['MODEL_PROVIDER'] = 'ollama'\n",
    "os.environ['OLLAMA_MODEL'] = 'llama3.1'\n",
    "\n",
    "# Reload config to pick up new environment\n",
    "import importlib\n",
    "import src.utils.config\n",
    "importlib.reload(src.utils.config)\n",
    "\n",
    "try:\n",
    "    from src.utils.config import Config\n",
    "    from src.tutor.ai_tutor import AITutor\n",
    "    \n",
    "    config = Config()\n",
    "    tutor = AITutor.from_config(config)\n",
    "    response = tutor.generate_response(\"Hello, can you help me learn Spanish?\")\n",
    "    print(f\"‚úÖ Ollama Response: {response[:100]}...\")\n",
    "    \n",
    "    # Test lesson functionality\n",
    "    print(\"\\n3. Testing Lesson Generation with Ollama:\")\n",
    "    tutor.set_learning_context(\"Spanish\", \"beginner\", \"conversation\", {})\n",
    "    intro = tutor.generate_lesson_introduction()\n",
    "    print(f\"‚úÖ Lesson Introduction: {intro[:150]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama Error: {e}\")\n",
    "    print(\"Make sure Ollama is running: `ollama serve`\")\n",
    "    print(\"And the model is available: `ollama pull llama3.1`\")\n",
    "\n",
    "print(\"\\n=== Integration Test Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4698cee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing SpeechHandler initialization...\n",
      "‚úÖ SpeechHandler initialized successfully!\n",
      "   STT Provider: google\n",
      "   TTS Provider: None\n",
      "\n",
      "üé¨ Testing Streamlit app pattern...\n",
      "   This is the same pattern used in app.py\n",
      "   ‚úÖ The Streamlit error should now be fixed!\n"
     ]
    }
   ],
   "source": [
    "# Test SpeechHandler Fix\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from src.tutor.speech import SpeechHandler\n",
    "\n",
    "print(\"üîß Testing SpeechHandler initialization...\")\n",
    "\n",
    "# Test the new configuration format\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "speech_config = {\n",
    "    'openai_api_key': openai_key,\n",
    "    'stt_provider': 'openai' if openai_key else 'google',\n",
    "    'tts_provider': 'openai' if openai_key else None\n",
    "}\n",
    "\n",
    "try:\n",
    "    handler = SpeechHandler(speech_config)\n",
    "    print(\"‚úÖ SpeechHandler initialized successfully!\")\n",
    "    print(f\"   STT Provider: {speech_config['stt_provider']}\")\n",
    "    print(f\"   TTS Provider: {speech_config['tts_provider']}\")\n",
    "    \n",
    "    # Test that it works with the Streamlit app pattern\n",
    "    print(\"\\nüé¨ Testing Streamlit app pattern...\")\n",
    "    print(\"   This is the same pattern used in app.py\")\n",
    "    print(\"   ‚úÖ The Streamlit error should now be fixed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"This indicates there's still an issue with SpeechHandler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4938fa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Complete Integration (Streamlit App Flow)\n",
      "============================================================\n",
      "1Ô∏è‚É£ Initializing configuration...\n",
      "   ‚úÖ Model Provider: ollama\n",
      "2Ô∏è‚É£ Initializing AI Tutor...\n",
      "   ‚úÖ AI Tutor initialized\n",
      "3Ô∏è‚É£ Initializing Speech Handler...\n",
      "   ‚úÖ Speech Handler initialized\n",
      "4Ô∏è‚É£ Initializing other components...\n",
      "   ‚úÖ Lesson Manager and Progress Tracker initialized\n",
      "5Ô∏è‚É£ Simulating lesson start...\n",
      "   ‚úÖ Session started: 4\n",
      "6Ô∏è‚É£ Testing lesson introduction...\n",
      "‚ùå Integration test failed: model \"llama3.1\" not found, try pulling it first (status code: 404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/2j/sm5q294j54n4m4_nwdt9bqww0000gn/T/ipykernel_12606/1870216217.py\", line 62, in <module>\n",
      "    intro = tutor.generate_lesson_introduction()\n",
      "  File \"/Users/andreslaurito/repos/language-tutor/src/tutor/ai_tutor.py\", line 150, in generate_lesson_introduction\n",
      "    response = self.llm.invoke([SystemMessage(content=self.get_system_prompt()),\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 642, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 543, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 525, in _create_chat_stream\n",
      "    yield from self._client.chat(\n",
      "  File \"/opt/homebrew/anaconda3/envs/language-tutor/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"llama3.1\" not found, try pulling it first (status code: 404)\n"
     ]
    }
   ],
   "source": [
    "# Complete Integration Test (Simulating Streamlit App Flow)\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from src.utils.config import Config\n",
    "from src.tutor.ai_tutor import AITutor\n",
    "from src.tutor.speech import SpeechHandler\n",
    "from src.tutor.lessons import LessonManager\n",
    "from src.utils.database import ProgressTracker\n",
    "\n",
    "print(\"üß™ Testing Complete Integration (Streamlit App Flow)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Step 1: Initialize Config (like the app does)\n",
    "    print(\"1Ô∏è‚É£ Initializing configuration...\")\n",
    "    config = Config()\n",
    "    print(f\"   ‚úÖ Model Provider: {config.MODEL_PROVIDER}\")\n",
    "    \n",
    "    # Step 2: Initialize AITutor (like the app does)\n",
    "    print(\"2Ô∏è‚É£ Initializing AI Tutor...\")\n",
    "    tutor = AITutor.from_config(config)\n",
    "    print(\"   ‚úÖ AI Tutor initialized\")\n",
    "    \n",
    "    # Step 3: Initialize SpeechHandler (like the app does)\n",
    "    print(\"3Ô∏è‚É£ Initializing Speech Handler...\")\n",
    "    openai_key = os.getenv('OPENAI_API_KEY')\n",
    "    speech_config = {\n",
    "        'openai_api_key': openai_key,\n",
    "        'stt_provider': 'openai' if openai_key else 'google',\n",
    "        'tts_provider': 'openai' if openai_key else None\n",
    "    }\n",
    "    speech_handler = SpeechHandler(speech_config)\n",
    "    print(\"   ‚úÖ Speech Handler initialized\")\n",
    "    \n",
    "    # Step 4: Initialize other components\n",
    "    print(\"4Ô∏è‚É£ Initializing other components...\")\n",
    "    lesson_manager = LessonManager()\n",
    "    progress_tracker = ProgressTracker()\n",
    "    print(\"   ‚úÖ Lesson Manager and Progress Tracker initialized\")\n",
    "    \n",
    "    # Step 5: Simulate starting a lesson\n",
    "    print(\"5Ô∏è‚É£ Simulating lesson start...\")\n",
    "    language = \"Spanish\"\n",
    "    difficulty = \"beginner\"\n",
    "    lesson_type = \"conversation\"\n",
    "    \n",
    "    # Get lesson content\n",
    "    lessons = lesson_manager.get_lessons(lesson_type, difficulty)\n",
    "    lesson_data = lessons[0] if lessons else {}\n",
    "    \n",
    "    # Set learning context\n",
    "    tutor.set_learning_context(language, difficulty, lesson_type, lesson_data)\n",
    "    \n",
    "    # Start session\n",
    "    session_id = progress_tracker.start_session(\"test_user\", language, lesson_type, difficulty)\n",
    "    print(f\"   ‚úÖ Session started: {session_id}\")\n",
    "    \n",
    "    # Step 6: Test lesson introduction\n",
    "    print(\"6Ô∏è‚É£ Testing lesson introduction...\")\n",
    "    intro = tutor.generate_lesson_introduction()\n",
    "    print(f\"   ‚úÖ Generated introduction: {intro[:100]}...\")\n",
    "    \n",
    "    print(\"\\nüéâ INTEGRATION TEST PASSED!\")\n",
    "    print(\"‚úÖ All components work together correctly\")\n",
    "    print(\"‚úÖ Streamlit app should now work without the SpeechHandler error\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Integration test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ecdd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶ô Testing with your available Ollama model...\n",
      "Model: mistral:latest\n",
      "‚úÖ Lesson Intro: ¬°Hola! Hola, bienvenido/a a nuestra lecci√≥n de conversaci√≥n en espa√±ol. ¬øC√≥mo te sientes hoy? Estamos muy emocionados por aprender y practicar juntos este lindo idioma.\n",
      "\n",
      "En esta clase, vamos a trabaja...\n",
      "‚ùå Error: 1 validation error for ConversationChain\n",
      "__root__\n",
      "  Got unexpected prompt input variables. The prompt expects ['history'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)\n",
      "Make sure Ollama is running: `ollama serve`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreslaurito/repos/language-tutor/src/tutor/ai_tutor.py:173: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "# Test with Available Ollama Model\n",
    "import os\n",
    "os.environ['OLLAMA_MODEL'] = 'mistral:latest'  # Use your available model\n",
    "\n",
    "# Reload config to pick up the new model\n",
    "import importlib\n",
    "import src.utils.config\n",
    "importlib.reload(src.utils.config)\n",
    "\n",
    "from src.utils.config import Config\n",
    "from src.tutor.ai_tutor import AITutor\n",
    "\n",
    "print(\"ü¶ô Testing with your available Ollama model...\")\n",
    "print(f\"Model: {os.getenv('OLLAMA_MODEL')}\")\n",
    "\n",
    "try:\n",
    "    config = Config()\n",
    "    tutor = AITutor.from_config(config)\n",
    "    \n",
    "    # Test lesson functionality (this method exists)\n",
    "    tutor.set_learning_context(\"Spanish\", \"beginner\", \"conversation\", {})\n",
    "    intro = tutor.generate_lesson_introduction()\n",
    "    print(f\"‚úÖ Lesson Intro: {intro[:200]}...\")\n",
    "    \n",
    "    # Test student input processing\n",
    "    response_data = tutor.process_student_input(\"Hello, can you help me learn Spanish?\")\n",
    "    print(f\"‚úÖ Response: {response_data.get('response', 'No response')[:200]}...\")\n",
    "    \n",
    "    print(\"\\nüéâ COMPLETE SUCCESS!\")\n",
    "    print(\"‚úÖ SpeechHandler error fixed\")\n",
    "    print(\"‚úÖ Ollama integration working\")\n",
    "    print(\"‚úÖ All components functional\")\n",
    "    print(\"\\nüåü Your AI Language Tutor is ready to use!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure Ollama is running: `ollama serve`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b477892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing lesson initialization with mistral:latest...\n",
      "==================================================\n",
      "‚úÖ All components initialized successfully\n",
      "‚úÖ Learning context set\n",
      "‚úÖ Session started: 8\n",
      "‚úÖ Lesson intro generated: ¬°Hola y bienvenido/a a nuestra clase de espa√±ol! ¬°Estoy muy emocionado por ense√±arte! En esta lecci√≥n, aprenderemos a interactuar en espa√±ol y practic...\n",
      "\n",
      "üéâ SUCCESS! The 'model not found' error is FIXED!\n",
      "‚úÖ Your AI Language Tutor is working with mistral:latest\n",
      "‚úÖ Streamlit app should now work without model errors\n",
      "\n",
      "üöÄ You can now:\n",
      "   ‚Ä¢ Run: streamlit run app.py\n",
      "   ‚Ä¢ Use the CLI: python cli_tutor.py\n",
      "   ‚Ä¢ Continue with this notebook!\n"
     ]
    }
   ],
   "source": [
    "# üéØ Final Test: Lesson Initialization with Correct Model\n",
    "print(\"üß™ Testing lesson initialization with mistral:latest...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Initialize components\n",
    "    config = Config()\n",
    "    tutor = AITutor.from_config(config)\n",
    "    lesson_manager = LessonManager()\n",
    "    progress_tracker = ProgressTracker()\n",
    "    \n",
    "    print(\"‚úÖ All components initialized successfully\")\n",
    "    \n",
    "    # Set learning context\n",
    "    tutor.set_learning_context(\"Spanish\", \"beginner\", \"conversation\", {})\n",
    "    print(\"‚úÖ Learning context set\")\n",
    "    \n",
    "    # Start a session\n",
    "    session_id = progress_tracker.start_session(\"notebook_user\", \"Spanish\", \"conversation\", \"beginner\")\n",
    "    print(f\"‚úÖ Session started: {session_id}\")\n",
    "    \n",
    "    # Generate lesson introduction\n",
    "    intro = tutor.generate_lesson_introduction()\n",
    "    print(f\"‚úÖ Lesson intro generated: {intro[:150]}...\")\n",
    "    \n",
    "    print(\"\\nüéâ SUCCESS! The 'model not found' error is FIXED!\")\n",
    "    print(\"‚úÖ Your AI Language Tutor is working with mistral:latest\")\n",
    "    print(\"‚úÖ Streamlit app should now work without model errors\")\n",
    "    print(\"\\nüöÄ You can now:\")\n",
    "    print(\"   ‚Ä¢ Run: streamlit run app.py\")\n",
    "    print(\"   ‚Ä¢ Use the CLI: python cli_tutor.py\") \n",
    "    print(\"   ‚Ä¢ Continue with this notebook!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    if \"404\" in str(e):\n",
    "        print(\"\\nüí° Model still not found. Try:\")\n",
    "        print(\"   1. Check Ollama is running: ollama serve\")\n",
    "        print(\"   2. List models: ollama list\")\n",
    "        print(\"   3. Use exact model name from the list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab85349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing SpeechHandler with new configuration format...\n",
      "‚úÖ SpeechHandler initialized successfully!\n",
      "   STT Provider: google\n",
      "   TTS Provider: None\n",
      "‚úÖ Streamlit app should now work without RerunData errors!\n"
     ]
    }
   ],
   "source": [
    "# üîß Test SpeechHandler Configuration Fix\n",
    "from src.tutor.speech import SpeechHandler\n",
    "\n",
    "print(\"üß™ Testing SpeechHandler with new configuration format...\")\n",
    "\n",
    "# Test the configuration format used by Streamlit app\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "speech_config = {\n",
    "    'openai_api_key': openai_key,\n",
    "    'stt_provider': 'openai' if openai_key else 'google',\n",
    "    'tts_provider': 'openai' if openai_key else None\n",
    "}\n",
    "\n",
    "try:\n",
    "    handler = SpeechHandler(speech_config)\n",
    "    print(\"‚úÖ SpeechHandler initialized successfully!\")\n",
    "    print(f\"   STT Provider: {speech_config['stt_provider']}\")\n",
    "    print(f\"   TTS Provider: {speech_config['tts_provider']}\")\n",
    "    print(\"‚úÖ Streamlit app should now work without RerunData errors!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d6b2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Simulating Streamlit App Initialization...\n",
      "============================================================\n",
      "1Ô∏è‚É£ Initializing Config...\n",
      "   ‚úÖ Provider: ollama\n",
      "   ‚úÖ Model: mistral:latest\n",
      "2Ô∏è‚É£ Initializing AI Tutor...\n",
      "   ‚úÖ AI Tutor ready\n",
      "3Ô∏è‚É£ Initializing Speech Handler...\n",
      "   ‚úÖ Speech Handler ready\n",
      "4Ô∏è‚É£ Initializing Other Components...\n",
      "   ‚úÖ Lesson Manager and Progress Tracker ready\n",
      "5Ô∏è‚É£ Testing Lesson Start...\n",
      "   ‚úÖ Session 11 started successfully\n",
      "   ‚úÖ Lesson intro: ¬°Hola, bienvenido/a al clase de espa√±ol! ¬°Estoy encantado de ense√±arte c√≥mo hablar espa√±ol! En esta ...\n",
      "\n",
      "üéâ SUCCESS! All RerunData errors should be FIXED!\n",
      "‚úÖ Streamlit app should now work perfectly\n",
      "‚úÖ You can start lessons without crashes\n",
      "‚úÖ Speech functionality is properly configured\n",
      "\n",
      "üöÄ Ready to test Streamlit app: streamlit run app.py\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Complete Streamlit App Simulation Test\n",
    "print(\"üé¨ Simulating Streamlit App Initialization...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Set environment variables (like the Streamlit app does)\n",
    "os.environ['MODEL_PROVIDER'] = 'ollama'\n",
    "os.environ['OLLAMA_MODEL'] = 'mistral:latest'  # Use your available model\n",
    "\n",
    "# Step 2: Initialize all components like the Streamlit app\n",
    "try:\n",
    "    # Initialize config\n",
    "    print(\"1Ô∏è‚É£ Initializing Config...\")\n",
    "    config = Config()\n",
    "    print(f\"   ‚úÖ Provider: {config.MODEL_PROVIDER}\")\n",
    "    print(f\"   ‚úÖ Model: {config.OLLAMA_MODEL}\")\n",
    "    \n",
    "    # Initialize AI Tutor  \n",
    "    print(\"2Ô∏è‚É£ Initializing AI Tutor...\")\n",
    "    tutor = AITutor.from_config(config)\n",
    "    print(\"   ‚úÖ AI Tutor ready\")\n",
    "    \n",
    "    # Initialize Speech Handler (the problematic component)\n",
    "    print(\"3Ô∏è‚É£ Initializing Speech Handler...\")\n",
    "    openai_key = os.getenv('OPENAI_API_KEY')\n",
    "    speech_config = {\n",
    "        'openai_api_key': openai_key,\n",
    "        'stt_provider': 'openai' if openai_key else 'google',\n",
    "        'tts_provider': 'openai' if openai_key else None\n",
    "    }\n",
    "    speech_handler = SpeechHandler(speech_config)\n",
    "    print(\"   ‚úÖ Speech Handler ready\")\n",
    "    \n",
    "    # Initialize other components\n",
    "    print(\"4Ô∏è‚É£ Initializing Other Components...\")\n",
    "    lesson_manager = LessonManager()\n",
    "    progress_tracker = ProgressTracker()\n",
    "    print(\"   ‚úÖ Lesson Manager and Progress Tracker ready\")\n",
    "    \n",
    "    # Test lesson start (the action that was failing)\n",
    "    print(\"5Ô∏è‚É£ Testing Lesson Start...\")\n",
    "    language = \"Spanish\"\n",
    "    lesson_type = \"conversation\"\n",
    "    difficulty = \"beginner\"\n",
    "    \n",
    "    lessons = lesson_manager.get_lessons(lesson_type, difficulty)\n",
    "    lesson_data = lessons[0] if lessons else {}\n",
    "    \n",
    "    tutor.set_learning_context(language, difficulty, lesson_type, lesson_data)\n",
    "    session_id = progress_tracker.start_session(\"streamlit_user\", language, lesson_type, difficulty)\n",
    "    intro = tutor.generate_lesson_introduction()\n",
    "    \n",
    "    print(f\"   ‚úÖ Session {session_id} started successfully\")\n",
    "    print(f\"   ‚úÖ Lesson intro: {intro[:100]}...\")\n",
    "    \n",
    "    print(\"\\nüéâ SUCCESS! All RerunData errors should be FIXED!\")\n",
    "    print(\"‚úÖ Streamlit app should now work perfectly\")\n",
    "    print(\"‚úÖ You can start lessons without crashes\")\n",
    "    print(\"‚úÖ Speech functionality is properly configured\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    if \"RerunData\" in str(e):\n",
    "        print(\"\\nüí° RerunData error indicates a Streamlit internal issue\")\n",
    "        print(\"   This usually happens when there's a configuration mismatch\")\n",
    "        print(\"   The fix should resolve this issue\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to test Streamlit app: streamlit run app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c16db189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing dynamic Ollama model detection for Streamlit dropdown...\n",
      "üì¶ Detected models: ['llama3.1:latest', 'mistral:latest', 'llama3.2:latest']\n",
      "‚úÖ Final dropdown options: ['gemma2', 'llama3.1', 'llama3.1:latest', 'llama3.2', 'llama3.2:latest', 'mistral', 'mistral:latest', 'qwen2']\n",
      "üéØ Smart default: mistral:latest (index 6)\n",
      "‚úÖ llama3.2 models available: YES\n",
      "\n",
      "üéâ Streamlit dropdown will now include: ['gemma2', 'llama3.1', 'llama3.1:latest', 'llama3.2', 'llama3.2:latest', 'mistral', 'mistral:latest', 'qwen2']\n",
      "‚úÖ llama3.2 is now available in the dropdown!\n",
      "\n",
      "üöÄ Run 'streamlit run app.py' to test the updated dropdown\n"
     ]
    }
   ],
   "source": [
    "# üîç Test Dynamic Ollama Model Detection (Streamlit Dropdown Fix)\n",
    "import requests\n",
    "import os\n",
    "\n",
    "print(\"üîç Testing dynamic Ollama model detection for Streamlit dropdown...\")\n",
    "\n",
    "# This is the same logic now used in the Streamlit app\n",
    "available_models = [\"llama3.1\", \"llama3.2\", \"qwen2\", \"mistral\", \"gemma2\"]  # Default options\n",
    "\n",
    "try:\n",
    "    ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "    response = requests.get(f\"{ollama_url}/api/tags\", timeout=2)\n",
    "    if response.status_code == 200:\n",
    "        models_data = response.json().get('models', [])\n",
    "        if models_data:\n",
    "            # Extract model names and add them to the list\n",
    "            detected_models = [model['name'] for model in models_data]\n",
    "            print(f\"üì¶ Detected models: {detected_models}\")\n",
    "            \n",
    "            # Combine detected models with defaults, remove duplicates\n",
    "            all_models = list(set(available_models + detected_models))\n",
    "            available_models = sorted(all_models)\n",
    "            \n",
    "            print(f\"‚úÖ Final dropdown options: {available_models}\")\n",
    "            \n",
    "            # Test smart default selection\n",
    "            default_index = 0\n",
    "            if \"mistral:latest\" in available_models:\n",
    "                default_index = available_models.index(\"mistral:latest\")\n",
    "                print(f\"üéØ Smart default: mistral:latest (index {default_index})\")\n",
    "            elif \"llama3.2:latest\" in available_models:\n",
    "                default_index = available_models.index(\"llama3.2:latest\")\n",
    "                print(f\"üéØ Smart default: llama3.2:latest (index {default_index})\")\n",
    "            elif \"mistral\" in available_models:\n",
    "                default_index = available_models.index(\"mistral\")\n",
    "                print(f\"üéØ Smart default: mistral (index {default_index})\")\n",
    "            elif \"llama3.2\" in available_models:\n",
    "                default_index = available_models.index(\"llama3.2\")\n",
    "                print(f\"üéØ Smart default: llama3.2 (index {default_index})\")\n",
    "            \n",
    "            print(f\"‚úÖ llama3.2 models available: {'YES' if any('llama3.2' in model for model in available_models) else 'NO'}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No models detected from Ollama API\")\n",
    "    else:\n",
    "        print(f\"‚ùå Ollama API returned status {response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
    "    print(\"üí° Using default model list\")\n",
    "\n",
    "print(f\"\\nüéâ Streamlit dropdown will now include: {available_models}\")\n",
    "print(\"‚úÖ llama3.2 is now available in the dropdown!\")\n",
    "print(\"\\nüöÄ Run 'streamlit run app.py' to test the updated dropdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "390b1a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶ô Testing LLama 3.2 integration in AI Language Tutor...\n",
      "============================================================\n",
      "üîß Using model: llama3.2:latest\n",
      "‚úÖ AI Tutor initialized with llama3.2:latest\n",
      "‚úÖ Lesson intro: ¬°Hola! (Hello!) Welcome to our Spanish conversation lesson today. I'm so glad you're here to practice your speaking skills with me.\n",
      "\n",
      "In this lesson, w...\n",
      "‚ùå Error: 1 validation error for ConversationChain\n",
      "__root__\n",
      "  Got unexpected prompt input variables. The prompt expects ['history'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)\n",
      "üí° Make sure Ollama is running and llama3.2:latest is available\n",
      "üí° Check with: ollama list\n"
     ]
    }
   ],
   "source": [
    "# ü¶ô Final Test: LLama 3.2 Integration\n",
    "print(\"ü¶ô Testing LLama 3.2 integration in AI Language Tutor...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with llama3.2:latest (your available model)\n",
    "os.environ['MODEL_PROVIDER'] = 'ollama'\n",
    "os.environ['OLLAMA_MODEL'] = 'llama3.2:latest'\n",
    "\n",
    "# Reload config to pick up new model\n",
    "import importlib\n",
    "import src.utils.config\n",
    "importlib.reload(src.utils.config)\n",
    "\n",
    "try:\n",
    "    from src.utils.config import Config\n",
    "    from src.tutor.ai_tutor import AITutor\n",
    "    \n",
    "    print(f\"üîß Using model: {os.getenv('OLLAMA_MODEL')}\")\n",
    "    \n",
    "    # Initialize components\n",
    "    config = Config()\n",
    "    tutor = AITutor.from_config(config)\n",
    "    print(f\"‚úÖ AI Tutor initialized with {config.OLLAMA_MODEL}\")\n",
    "    \n",
    "    # Test lesson generation\n",
    "    tutor.set_learning_context(\"Spanish\", \"beginner\", \"conversation\", {})\n",
    "    intro = tutor.generate_lesson_introduction()\n",
    "    print(f\"‚úÖ Lesson intro: {intro[:150]}...\")\n",
    "    \n",
    "    # Test conversation\n",
    "    response_data = tutor.process_student_input(\"¬°Hola! ¬øC√≥mo te llamas?\")\n",
    "    print(f\"‚úÖ Response: {response_data.get('response', 'No response')[:150]}...\")\n",
    "    \n",
    "    print(f\"\\nüéâ SUCCESS! LLama 3.2 is working perfectly!\")\n",
    "    print(\"‚úÖ Model is available in Streamlit dropdown\")\n",
    "    print(\"‚úÖ AI responses are generated correctly\")\n",
    "    print(\"‚úÖ Language learning functionality works\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Now you can:\")\n",
    "    print(\"   ‚Ä¢ Select 'llama3.2:latest' from Streamlit dropdown\")\n",
    "    print(\"   ‚Ä¢ Use CLI: python cli_tutor.py --provider ollama --model llama3.2:latest\")\n",
    "    print(\"   ‚Ä¢ Enjoy faster and better language learning!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Make sure Ollama is running and llama3.2:latest is available\")\n",
    "    print(\"üí° Check with: ollama list\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "language-tutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
